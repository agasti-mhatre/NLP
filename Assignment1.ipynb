{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\agasti mhatre\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\agasti mhatre\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\agasti mhatre\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\agasti mhatre\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\agasti mhatre\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\agasti mhatre\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK for tokenization algorithms\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Agasti\n",
      "[nltk_data]     Mhatre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Agasti\n",
      "[nltk_data]     Mhatre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries for corpora training and for using NLTK library.\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put each token in a linked list node. This way, when merges happen,\n",
    "# the node after it can be removed in O(1) time and the new merged token\n",
    "# can happen in the current node.\n",
    "class ListNode:\n",
    "\n",
    "    def __init__(self, val):\n",
    "\n",
    "        self.val = val\n",
    "        self.next = None\n",
    "\n",
    "def string_to_linked_list(curr_string):\n",
    "\n",
    "    # Head of the linked List\n",
    "    l_string = curr = None\n",
    "    c = 0\n",
    "\n",
    "    # Skip over any leading whitespace in the string\n",
    "    while c < len(curr_string):\n",
    "\n",
    "        if curr_string[c] not in [' ', '\\n', '\\t']: break\n",
    "        c += 1\n",
    "        \n",
    "    # Turn string in a linked list where each node\n",
    "    # is either a letter or an underscore (remove whitespace)\n",
    "    while c < len(curr_string): \n",
    "    \n",
    "        # If the current letter in the string is a whitespace, turn\n",
    "        # it into an underscore and put it in the linked list\n",
    "        # Otherwise, continue on in the string and add each\n",
    "        # letter to a linked list node\n",
    "        addUnderScore = False\n",
    "        while c < len(curr_string):\n",
    "            \n",
    "            if curr_string[c] not in [' ', '\\n', '\\t']: break\n",
    "            else: addUnderScore = True\n",
    "            \n",
    "            c += 1\n",
    "\n",
    "        if addUnderScore:\n",
    "\n",
    "            curr.next = ListNode('_')\n",
    "            curr = curr.next\n",
    "\n",
    "        if c >= len(curr_string): break\n",
    "\n",
    "        # Set the head of the l_string linked list\n",
    "        # if it is not currently set.\n",
    "        # Otherwise, add another child node to the\n",
    "        # linked list\n",
    "        if l_string == None: \n",
    "            \n",
    "            l_string = ListNode(curr_string[c])\n",
    "            curr = l_string\n",
    "\n",
    "        else:\n",
    "\n",
    "            curr.next = ListNode(curr_string[c])\n",
    "            curr = curr.next\n",
    "\n",
    "        c += 1\n",
    "\n",
    "    return l_string\n",
    "\n",
    "def merge_rules(l_string, x, y): \n",
    "        \n",
    "    temp = x + y\n",
    "    curr = l_string\n",
    "    while curr != None:\n",
    "\n",
    "        if curr.next == None: break\n",
    "        if (x == curr.val) and (y == curr.next.val):\n",
    "\n",
    "            curr.next = curr.next.next\n",
    "            curr.val = temp\n",
    "\n",
    "        curr = curr.next\n",
    "\n",
    "def extract_vocabulary(l_string):\n",
    "\n",
    "    # Each list node is a token\n",
    "    # that can now be part of the\n",
    "    # vocabulary\n",
    "    temp = set()\n",
    "    curr = l_string\n",
    "    while curr != None:\n",
    "\n",
    "        temp.add(curr.val)\n",
    "        curr = curr.next\n",
    "\n",
    "    # Remove underscores from each token\n",
    "    vocabulary = set()\n",
    "    for token in temp:\n",
    "\n",
    "        i = 0\n",
    "        j = len(token)\n",
    "        if token[i] == '_': i += 1\n",
    "        if token[j - 1] == '_': j -= 1\n",
    "\n",
    "        vocabulary.add(\" \".join(token[i:j].split(\"_\")))\n",
    "\n",
    "    return vocabulary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Implement BPE Algorithm\n",
    "\n",
    "# Find rules to merge tokens. Take a test string\n",
    "# and run BPE algorithm on it (for k iterations) \n",
    "# to find the rules and generate the vocabulary.\n",
    "def trainBPE(train_string, k):\n",
    "\n",
    "    rules = []\n",
    "\n",
    "    l_string = string_to_linked_list(train_string)\n",
    "    \n",
    "    # Run the BPE algorithm for k iterations\n",
    "    for i in range(k):\n",
    "\n",
    "        # Store the frequency of the current iteration's\n",
    "        # most common pairs that occur together\n",
    "        freq_ = defaultdict(lambda: 0)\n",
    "        curr = l_string\n",
    "        while curr.next != None: \n",
    "\n",
    "            freq_[(curr.val, curr.next.val)] += 1\n",
    "            curr = curr.next\n",
    "\n",
    "        # Find the pair of tokens which occurs \n",
    "        # together the most frequently\n",
    "        max_rule = None\n",
    "        max_num = 0\n",
    "        for rule, num in freq_.items():\n",
    "\n",
    "            if num > max_num:\n",
    "\n",
    "                max_rule = rule\n",
    "                max_num = num\n",
    "\n",
    "        # If there is no rule that is found, stop the\n",
    "        # algorithm\n",
    "        if max_rule == None: break\n",
    "\n",
    "        # Go through the string and merge the most frequent \n",
    "        # pair according to the rule found\n",
    "        x, y = max_rule\n",
    "        rules.append((x, y))\n",
    "        merge_rules(l_string, x, y)\n",
    "\n",
    "    return extract_vocabulary(l_string), rules\n",
    "\n",
    "# Test on the following string for k=17 iterations\n",
    "#vocabulary, rules = trainBPE(\"low low low lowest lowest newer newer wider wider new new\", 17)\n",
    "\n",
    "#print(\"Vocabulary: \", vocabulary)\n",
    "#print(\"Rules: \", rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testBPE(test_string, rules):\n",
    "\n",
    "    l_string = string_to_linked_list(test_string)\n",
    "    for rule in rules:\n",
    "\n",
    "        x, y = rule\n",
    "        merge_rules(l_string, x, y)\n",
    "\n",
    "    return extract_vocabulary(l_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'', 'mi', 's', 'in', 'they', 'lo', '4', 'pos', ', as', '0', 'der', 'ish', 'ch', 'most', '&', 'you', '1', 'ha', 'ful', 'ith', 'art', \"'s\", 'z', 'um', '. The', 'A', 'i', 'ran', 'Jan', 'ci', 'do', 'id', 'y', 'in the', 'w', '. She', 'might', 'long', 'Y', 'st', 'would', 'ction', 'im', 'G', 'ood', 'e the', 'L', 'ter', 'ent', 'noth', 'ev', 'ak', 'O', 'pe', 'er', 'will', 'fort', 'his', 'Mrs.', ', th', 'pect', ')', 'have', 'fi', 'were', 'Harriet', 'fri', 'from', \"'\", 'now', 'such', '-', 'F', 't of', 'as', 'con', 't', 'ir', 'ad', 'Fran', 'P', 'not', 'M', 'her', 'may', 'ar', 'int', 'ard', 'ur', 'though', 'little', 'is', 'ver', 'a', 'g', 'ou', 'which', 'to be', 'est', 'of the', '`', ';', 'o', 'rem', '. I', 'pa', 'm', 'I', 'But', 'da', '3', 'to', '. T', 'ct', 'Weston', 'hi', 'e to', 'being', 'befor', 'able', 'and', 'ation', 'en', 'pro', 'arri', '(', 'e', 'et', 'Chur', 'iss', 'erv', 'f', 'only', 'U', 'self', 'feel', 'K', 'pre', '; but', 'ul', 'good', 'ere', 'chi', 'it', 'ear', 'could', 'si', 'mor', 'th', 'dis', 'oo', 'll', 'wa', 'ing', 'all', 'ro', 'res', 'Woodhou', ', that', 'ed,', 'fer', 'he', 'said', 'ng', 'this', 'has', 'Knightle', 'pla', 'much', 'friend', 'ma', 'Fairfax', 'ely', 'great', 'night', 'any', ',\" said', 'ust', 'ell', 'pleas', '. \"', 'S', 'ow', 'di', 'ca', 'under', 'c', 'n', 'pp', 'ra', 'ke', 'ac', 'fir', 'hav', 'od', 'ple', 'la', 'what', 'there', 'been', 'bod', 'ion', 'af', 'tter', '.\" \"', 'ally', '. A', 'or', 'on', 'Churchi', 'ould', 'ic', 'could not', 'I am', 'comp', 'ive', 'an', 'Woodhouse', '!--', 'd', 'how', 'ry', 'fr', 'hou', 'gh', 'es,', 'ant', 'ol', 'hear', '2', '\" \"', 'here', 'so', 'way', 'cer', 'one', 'ld', 'wor', 'po', 'sib', 'em', 'cons', 'e th', 'Emma', 'gre', 'ill', 're', 'su', 'com', 'b', 'Em', 'Miss', '. H', '8', 's of', 'ome', 'our', 'y,', 'T', 'de', 'R', 'Elton', 'e,', 'oun', 'me', 'y th', 'their', '!', 'Mr.', ', wh', ':', 'per', 'other', 'ne', 'J', 'ough', 'X', 'ever', 'herself', 'op', 'hich', 'know', 'must', 'Mrs. Weston', 'own', 'hat', 'C', 'r', 'loo', 'p', ',\"', 'did', 'se', ',', 'ain', 'qu', 'u', 'iv', 'the', 'she', ', \"', ', to', '; and', 'Wes', 'lton', 've', 'ed to', 'that', 'B', 'thin', '.--', 'Harri', 'ure', '?', 'every', 'no', 'rat', 'man', ', the', 'sa', 'om', 'l', 'very', 'go', 'ut', 'le', 'fa', 'ist', 'ind', 'itt', '. It', 'v', 'think', 'She', 'E', 'of', 'never', 'we', 'e t', 'happ', 'es', 'ach', 'You', 'inde', 's.', 'was', 'hur', 'V', 'to the', 'ost', 'part', 'be', 'look', 'Mr. Knightle', 'a s', 'att', 'un', 'W', ', I', 'je', 'can', '.', '[', 'him', 'N', 'al', 'with', 'uch', 'ob', 'over', 'wh', '.\"', 'if', 'hould', '\"', 'pt', 'ight', 's,', 'Mr', 'pres', 'up', 'ong', 'by', '. He', 'air', 'han', 'ance', 'os', 'at', 'ce', ']', 'h', 'fe', 'Jane', 'ess', 'ri', 'el', 'app', 'for', 'am', 'them', ', she', 'young', 'fat', 'ex', 'ly', 'sel', 'ed', 'ite', 'your', 'ot', 'ous', 'Mr. Elton', 'il', 'Q', 'out', 'H', 'bu', '. W', 'again', 'but', 'x', '--', ', and', 'I s', 'ag', 'igh', 'had', 'har', 'my', '6', 'well', 'ton', 'li', 'ment', 'us', '7', 'pr', 'e of', 'ing to', 'j', 'more', 'quite', 'ab', 'end', 'of s', 'D', 'k', 'are', 'litt'}\n",
      "Rules:  [('e', '_'), ('_', 't'), ('_', 'a'), ('e', 'r'), ('t', '_'), ('i', 'n'), ('d', '_'), ('_t', 'h'), ('s', '_'), ('o', 'u'), (',', '_'), ('o', 'n'), ('y', '_'), ('.', '_'), ('o', '_'), ('e', 'n'), ('h', 'a'), ('_a', 'n'), ('in', 'g'), ('o', 'r'), ('a', 'n'), ('i', 't'), ('r', 'e'), ('o', 'f'), ('h', 'er'), ('_th', 'e_'), ('l', 'l'), ('h', 'e_'), ('a', 'r'), ('_', 's'), ('_t', 'o_'), ('n', 'o'), ('h', 'i'), ('e', 's'), ('a', 't'), ('w', 'a'), ('_an', 'd_'), ('-', '-'), ('e', 'd'), ('i', 's'), ('o', 'm'), ('v', 'er'), ('of', '_'), ('b', 'e'), ('ing', '_'), ('g', 'h'), ('e', 'd_'), ('e', 'l'), ('y', 'ou'), ('f', 'or'), ('c', 'h'), ('_a', '_'), ('I', '_'), ('e_', 't'), ('l', 'd_'), ('l', 'e'), ('her', '_'), ('i', 'on'), ('m', 'a'), ('ou', 'ld_'), ('M', 'r'), ('r', 'i'), ('v', 'e_'), ('u', 's'), ('b', 'e_'), ('no', 't_'), ('\"', '_'), ('wa', 's_'), ('a', 't_'), ('o', 'w'), ('it', 'h'), ('_', 'w'), ('i', 't_'), ('u', 'r'), ('ll', '_'), ('s', 't'), ('i', 'r'), (',', '_and_'), ('en', 't'), ('ver', 'y_'), ('o', 'o'), ('ha', 'd_'), ('d', 'e'), ('s', 'e'), ('_', 'm'), ('_th', 'at_'), ('l', 'i'), ('l', 'e_'), ('s', 'he_'), ('ha', 've_'), ('a', 'l'), ('b', 'u'), ('w', 'ith'), ('e', 'm'), ('c', 't'), ('i', 's_'), ('l', 'y_'), ('i', 'gh'), ('.', '\"_'), ('Mr', '._'), ('o', 's'), ('e', 't'), ('in', '_'), ('t', 'er'), ('hi', 's_'), ('_t', 'o'), ('a', 's'), ('en', '_'), ('c', 'om'), ('c', 'on'), ('s', '._'), ('n', 'e'), ('_an', 'd'), ('c', 'e'), ('h', 'e'), (';', '_'), ('u', 'ch'), ('hi', 'm'), ('u', 'n'), ('p', 'p'), ('_a', 's_'), ('q', 'u'), ('r', 'o'), ('bu', 't_'), ('t', 'on'), ('E', 'm'), ('Em', 'ma'), ('s', 'a'), ('er', 'e_'), ('ou', 'gh'), ('no', 't'), ('f', 'e'), ('k', '_'), ('no', 'w'), ('i', 'm'), ('w', 'h'), ('d', 'i'), ('re', 's'), ('.\"_', '\"'), ('a', 'b'), ('c', 'ould_'), ('c', 'e_'), ('o', 't'), ('l', 'a'), ('ou', 'r'), ('p', 'er'), ('at', 'ion'), ('.', '--'), ('l', 'y'), ('ar', 't'), (\"'\", 's_'), ('e', 'x'), ('wa', 's'), ('you', '_'), ('s', 'el'), ('a', 'll'), ('for', '_'), ('p', 'e'), ('Mr', 's._'), ('ar', 'ri'), ('f', 'a'), ('ha', 't_'), ('a', 'in'), ('u', 'l'), ('f', 'r'), ('w', 'ould_'), ('igh', 't'), ('sel', 'f'), ('us', 't_'), ('_th', 'e'), ('e', ',_'), ('is', 's_'), ('i', 'll_'), ('i', 'd'), ('i', 'c'), ('M', 'iss_'), ('e_t', 'o_'), ('en', 'd'), ('s', 'u'), (';', '_and_'), ('on', '_'), ('._', 'T'), ('S', 'he_'), ('g', 're'), ('w', 'e'), ('._', 'I'), ('an', '_'), ('hi', 'ch'), ('of', '_the_'), ('_a', 's'), ('_a', 't'), ('_a', 't_'), ('e', 's_'), ('_a', 'm'), ('_a', 'll'), ('e', 'ver'), ('fr', 'om'), ('a', 'd'), ('s', 'o_'), ('_s', 'he_'), ('m', 'or'), ('i', 'f'), ('n', '_'), ('k', 'now'), ('s', ',_'), ('u', 't'), ('._', 'H'), ('en', 't_'), ('_', 'of_'), ('_s', 'a'), ('y', ',_'), ('an', 'd_'), ('on', 'e_'), ('oo', 'd'), ('n', 'o_'), ('ed', '_to_'), ('or', '_'), ('it', 't'), ('an', 'd'), (',', '_and'), ('a', 'g'), ('o', 'p'), ('h', 'ou'), ('a', 'm'), ('_a', 'r'), ('H', 'arri'), ('i', 'd_'), ('e', 't_'), ('r', 'a'), ('e', 'v'), ('d', 'is'), ('n', 'ight'), ('with', '_'), ('es', 's'), ('k', 'e_'), ('in', 't'), ('W', 'es'), ('in', '_the_'), ('Wes', 'ton'), ('o', 'd'), ('!', '_'), ('l', 'd'), ('_th', 'in'), ('s', 'o'), ('ou', 't_'), ('p', 'ro'), ('w', 'hich'), ('i', 'v'), ('p', 'le'), ('on', 'g'), ('wa', 'y'), ('an', 't'), ('u', 'p'), ('d', 'o'), ('d', 'o_'), ('_th', 'em'), ('be', 'en'), ('m', 'e'), ('er', '_'), ('_a', 'l'), ('i', 'll'), ('l', 'ton'), ('m', 'uch'), ('E', 'lton'), ('m', 'y_'), ('you', 'r'), ('._', '\"'), ('_to_', 'be_'), ('d', 'a'), ('w', 'ere_'), ('y', '_th'), ('K', 'night'), ('Knight', 'le'), ('t', 'h'), ('h', 'ow'), ('._', 'She_'), ('ou', 'n'), ('ed', ',_'), ('e_t', 'he_'), ('_an', 'y_'), ('m', 'ust_'), ('ur', 'e_'), (',', '_th'), ('a', '_'), ('--', '_'), ('i', 'l'), ('in', 'd'), ('l', 'itt'), ('c', 'hi'), ('m', 'an'), ('is', 'h'), ('f', 'i'), ('it', 'e_'), ('Emma', '_'), ('._', 'I_'), ('ar', 'd'), ('be', 'en_'), ('ne', 'ver'), ('b', 'y_'), ('ha', 'n'), ('o', 'b'), ('r', 'an'), ('h', 'ould_'), ('p', 'os'), ('_a', 'b'), ('es', 't'), ('p', 'o'), ('igh', 't_'), ('f', 'ri'), ('w', 'ill_'), (';_', 'but_'), ('I', '_am'), ('w', 'or'), ('a', 'k'), ('ar', '_'), ('W', 'ood'), ('Wood', 'hou'), ('litt', 'le_'), ('t', 'ter'), ('om', 'e_'), ('es', 's_'), ('d', 'er'), ('_ar', 'e_'), ('p', 'art'), ('from', '_'), ('l', 'oo'), ('her', 'e_'), ('_a', 'g'), ('s', 'i'), ('Y', 'ou'), ('J', 'an'), ('p', 're'), ('Mr._', 'Knightle'), ('_sa', 'id_'), ('s', 'e_'), ('ha', 'v'), ('ha', 'pp'), ('_a', 'll_'), ('os', 't_'), ('l', 'o'), ('_a', '_s'), ('m', 'e_'), ('_a', 'c'), ('!', '--'), ('h', 'ur'), (';', '_and'), (',', '_that_'), ('Harri', 'et'), ('_the', 'y_'), ('el', 'y_'), ('u', 't_'), ('_an', '_'), ('ing', '_to_'), ('t_', 'of_'), ('ha', 'd'), ('c', 'a'), ('ple', 'as'), ('bu', 't'), (',', '\"'), ('a', 'ir'), ('._', 'A'), ('f', 'er'), ('her', 'self'), ('ot', 'her'), ('u', 'm'), (',_', '\"'), ('ab', 'le_'), ('r', 'at'), ('ct', 'ion'), ('_th', 'is_'), ('di', 'd_'), ('al', '_'), ('_the', 'ir'), ('oo', 'd_'), ('_a', 'f'), ('fe', 'el'), ('e', 'very_'), ('ou', 't'), ('r', 'y'), ('if', '_'), ('p', 'res'), ('b', 'od'), ('loo', 'k'), ('e', 'ar'), ('I', '_s'), ('._H', 'e_'), ('w', 'hat_'), ('p', 'la'), ('we', 'll'), ('which', '_'), ('Mrs._', 'Weston'), ('_w', 'ith'), ('your', '_'), ('fri', 'end'), ('all', 'y_'), (',_', 'I_'), ('qu', 'ite_'), ('of', '_s'), ('e_', 'of_'), ('ow', 'n'), ('be', 'for'), ('._I', 't_'), ('l', 'ong'), ('_th', 'ere_'), ('u', 're'), ('p', 'a'), ('could_', 'not_'), ('m', 'ost_'), ('us', 't'), ('m', 'ight_'), ('F', 'air'), ('Fair', 'fa'), ('Fairfa', 'x'), ('e_t', 'o'), ('o', 'ver'), ('m', 'ent'), ('C', 'hur'), ('Chur', 'chi'), ('f', 'ul'), ('not', 'h'), ('_at', 't'), ('ha', 'r'), ('ow', 'n_'), ('f', 'ir'), ('es', 't_'), ('_thin', 'k_'), ('s', 'uch'), ('on', 'ly_'), ('c', 'an'), ('_s', 'o_'), ('f', 'at'), ('Mr._', 'Elton'), ('_a', 'pp'), ('for', 't'), ('\"_', '\"'), ('w', '_'), ('._T', 'he_'), (',', '_the_'), ('_th', 'ough'), ('c', 'i'), ('gre', 'at_'), (',_', 'she_'), (',_', 'wh'), ('B', 'ut_'), ('c', 'er'), ('g', 'o'), ('g', 'ood_'), (',\"', '_said_'), ('d', 'e_'), ('n', 'g'), (',', '_as_'), ('p', 't'), ('._', 'W'), ('con', 's'), ('_a', 'd'), ('p', 'r'), ('_ag', 'ain'), ('.--', '_'), ('ot', 'her_'), ('b', 'y'), ('a', 'ch'), ('si', 'b'), ('s_', 'of_'), ('o', 'l'), ('him', '_'), ('in', 'de'), ('an', 'ce_'), ('j', 'e'), ('F', 'ran'), ('e_t', 'h'), ('Jan', 'e_'), ('pe', 'ct'), ('re', 'm'), ('e', 'll'), ('a', 'c'), ('Woodhou', 'se'), ('com', 'p'), ('never', '_'), ('_thin', 'k'), (',', '_to_'), ('_their', '_'), ('un', 'der'), ('is', 't'), ('be', 'ing_'), ('er', 'v'), ('m', 'i'), ('much', '_'), ('mor', 'e_'), ('he', 'ar'), ('you', 'ng'), ('ha', 's_'), ('_to', '_the_'), ('es', ',_'), ('ha', 't'), ('ar', 'e_'), ('ma', 'y_'), ('ou', 's'), ('i', 've_'), ('_th', 'at'), ('s', 't_')]\n"
     ]
    }
   ],
   "source": [
    "# 2. Train on NLTK Dataset\n",
    "\n",
    "# Store the training text\n",
    "text = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "vocabulary, rules = trainBPE(text, 500)\n",
    "\n",
    "print(\"Vocabulary: \", vocabulary)\n",
    "print(\"Rules: \", rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Test on NLTK Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create Reference Tokenization\n",
    "\n",
    "punkt_tokenizer = PunktSentenceTokenizer()\n",
    "tokens = punkt_tokenizer.tokenize(text)\n",
    "\n",
    "for tok in tokens: print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
